{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute hidden representations for a single input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch \n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "available_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name_or_path = \"princeton-nlp/Sheared-LLaMA-1.3B\"\n",
    "# model_name_or_path = \"meta-llama/Llama-2-7b-hf\"\n",
    "# model_name_or_path = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model_name_or_path = \"meta-llama/Meta-Llama-3-8B\"\n",
    "# model_name_or_path = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# model_name_or_path = \"mistralai/Mistral-7B-v0.1\"\n",
    "# model_name_or_path = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "# model_name_or_path = \"mistralai/Mistral-7B-Instruct-v0.2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify input text\n",
    "text_id, text = 'text-1', \"Montreal is the second most populous city in Canada, the tenth most populous city in North America, and the most populous city in the province of Quebec. Founded in 1642 as Ville-Marie, or 'City of Mary', it is named after Mount Royal, the triple-peaked hill around which the early city of Ville-Marie was built. The city is centred on the Island of Montreal, which obtained its name from the same origin as the city, and a few much smaller peripheral islands, the largest of which is ÃŽle Bizard. The city is 196 km (122 mi) east of the national capital, Ottawa, and 258 km (160 mi) southwest of the provincial capital, Quebec City.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for attention_type in ['causal', 'bidirectional']:\n",
    "    # load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "    attn_implementation = 'eager'\n",
    "    lm = AutoModelForCausalLM.from_pretrained(model_name_or_path, attn_implementation=attn_implementation)\n",
    "\n",
    "    # encode input text\n",
    "    ids = tokenizer.encode(text, padding=\"do_not_pad\")\n",
    "    tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "    seq_len = len(tokens)\n",
    "    input_ids = torch.tensor(ids).reshape(1, -1)\n",
    "    position_ids = torch.arange(start=0, end=seq_len).view(1, seq_len)\n",
    "\n",
    "    # enable bidirectional attention\n",
    "    attention_mask = None\n",
    "    if attention_type == \"bidirectional\":\n",
    "        # construct attention mask (batch_size, 1, seq_len, seq_len)\n",
    "        attention_mask = torch.ones(size=(1, 1, seq_len, seq_len)).to(device)\n",
    "\n",
    "        # for some models we need to overwrite the _update_causal_mask method\n",
    "        if model_name_or_path in [\"princeton-nlp/Sheared-LLaMA-1.3B\", \"meta-llama/Llama-2-7b-hf\", \"meta-llama/Llama-2-7b-chat-hf\", \"meta-llama/Meta-Llama-3-8B-Instruct\", \"meta-llama/Meta-Llama-3-8B\"]:\n",
    "            lm.model._update_causal_mask = lambda attention_mask, _: attention_mask\n",
    "        # for others it's sufficient to modify the attenion_mask when using attn_implementation == 'eager'\n",
    "\n",
    "    # run inference and return attentions as well as hidden states\n",
    "    lm.to(device)\n",
    "    input_ids = input_ids.to(device)\n",
    "    position_ids = position_ids.to(device)\n",
    "    labels = input_ids\n",
    "    output = lm.forward(input_ids=input_ids, position_ids=position_ids, labels=labels, attention_mask=attention_mask, output_attentions=True, output_hidden_states=True)\n",
    "\n",
    "    # SANITY CHECK: plot attention matrices\n",
    "    # A = output.attentions[-1].squeeze()[-1].detach().cpu().float().numpy() \n",
    "    # print(np.triu(A, k=1)) # the future. this should be all zeros when using causal attention and non-zero when using bidirectional attention\n",
    "\n",
    "    # save hidden states to disk\n",
    "    data_path = f\"/data/hidden_states_data/{model_name_or_path.split('/')[-1]}/{attention_type}/{text_id}\"\n",
    "    Path(data_path).mkdir(parents=True, exist_ok=True)    \n",
    "\n",
    "    # save hidden states of every layer\n",
    "    for layer in range(len(output.hidden_states)):\n",
    "        H = output.hidden_states[layer].detach().cpu().numpy()\n",
    "        file_name = f\"H_layer{layer}.npy\"\n",
    "        with open(os.path.join(data_path, file_name), 'wb') as f:\n",
    "            np.save(f, H)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
